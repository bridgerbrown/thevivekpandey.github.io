<html>
<head>
  <title> Notes on Cousera Deep Learning Course: Course 1</title>
  <link rel="stylesheet" type="text/css" href="style.css"/>
  <style>
     li {
        margin-bottom: 10px;
     }
  </style>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
</head>
<body>
<a href="http://thevivekpandey.github.io">Home</a>&nbsp;&nbsp; <a href="https://qgraph.io">QGraph</a>
<h1>Notes on Logistic Regression</h1>
<p>
I am doing Andrew Ng's course on deep learning, and summarising the lectures here. Feel free to send me pull request 
to make this page better.
</p>

<h2>Welcome</h2>
<p>
AI is new electricity, transforming numerous industries.
</p>

<h2>Neural Network</h2>
Looks like this<br>
<img src="neural-net.png" width=500/>
<br>
Multidimensional input goes to the neurons in first layer. Output of first layer neurons goes to second layer, and so on.

<br/>
Housing problem is structured, ads clicked or not is structured. Audio, image, and are unstructured.
<br/>
Deep learning is taking off now because there is lot of data to train on, and computing power to perform this training.

<h2>Logistic Regression</h2>
We are given $(x, y)$ pairs where $x \in R^{n_x}$ and $y \in \{0, 1\}$.
<br/> <br/>
$x$ is written as a column vector.
<br/> <br/>
We want $\hat{y} = P(y = 1 \mid x)$.
<br/> <br/>
We denote those pairs as $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) \dots (x^{(m)}, y^{(m)}) $
<br/> <br/>
We say $\hat{y} = \sigma(w^{T}x + b)$, or $\hat{y} = \sigma(z)$ where $z = w^{T}x + b$
<br/> <br/>
by which we mean
<br/> <br/>
$\hat{y^{(i)}} = \sigma(w^{T}x^{(i)} + b)$ for $i = 1\dots m$
<br/> <br/>
where $w \in R^{n_x}$ is again a column vector, and $b \in R$ and
$\sigma(z) = \frac{1}{1 + e^{-z}}$
<br/> <br/>
$w^T = [w_1, w_2, \dots w_{n_x}]$
<br/> <br/>
$\sigma(z)$ called "activation function". There can be many activation functions. They are the
ones which give rise to non linearity in logistic regression and neural networks.
<br/> <br/>

Now we need to find $w$ and $b$ such that cost function 
<br/> <br/>
$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y^{(i)}}, y^{i})$
<br/> <br/>
is minimized, where
<br/> <br/>
$L(\hat{y}, y) = -(ylog\hat{y} + (1 - y)log(1 - \hat{y}))$
<br/> <br/>
$J$ is called cost function, $L$ is called loss function.
<h2>Gradient Descent to Solve the Problem</h2>
Main idea is, start with some value of $w$ and $b$, and then repeatedly:
<br/> <br/>
$w_i = w_i - \alpha\frac{dJ(w,b)}{dw_i}$ for $i = 1\dots n_x$
<br/> <br/>
$b = b - \alpha\frac{dJ(w,b)}{db}$
<br/> <br/>
till $w$ and $b$ converge.
<br/> <br/>
Now, for one training example, let us use $\hat{y} = a$.
<br/> <br/>
$\frac{dL(a,y)}{dw_1} = \frac{dL(a,y)}{da}\frac{da}{dz}\frac{dz}{dw_1}$
<br/> <br/>
where $z = \sigma({w^{T}x + b})$
<br/> <br/>
It simplifies to
<br/> <br/>
$\frac{dL(a,y)}{dw_1} = (-\frac{y}{a} + \frac{1-y}{1-a}) * a(1-a) * x_1$
<br/> <br/>
$= (a - y)x_1$
<br/> <br/>
Similarly, we can find that
$\frac{dL}{db} = \frac{dL}{dz}$
<h4>Some notation</h4>
We denote $da = \frac{dL(a,y)}{da}$, $dz = \frac{dL(a,y)}{dz}$ and $dw = \frac{dL(a,y)}{dw}$
<br/> <br/>
Thus we write:
<br/> <br/>
$dz = (a - y)$, $dw_i = x_idz$
<br/> <br/>
Now, since you know $dw_i$, if there were just one training example, you could do
$w_i = w_i - \alpha dw_i$
<br/> <br/>
repeatedly till $w_i$ converged.
<br/> <br/>
But we have $m$ training examples.
<h3>Gradient descent on $m$ training examples</h3>
Thus, summing over $m$ training examples,
<br/> <br/>
$\frac{dJ}{dw_1} = \frac{1}{m}\Sigma_{i=1}^{m}\frac{d}{dw_1}L(a^i, y^i)$
$= \frac{1}{m}\Sigma_{i=1}^{m}dw_1^{(i)}$
<br/> <br/>
We call $dw_1 = \frac{dJ}{dw_1}$
<br/> <br/>
and set $w_1 = w_1 - \alpha dw_1$
<br/> <br/>
Similarly for $dw_2\dots dw_{n_x}$
<br/> <br/>
And $b = b - \alpha db$
<h2>Vectorizing Logistic Regression</h2>
Since $x$'s are written as column vector, we can introduce
<br/> <br/>
$X = 
\begin{bmatrix} 
\mid & \mid & & \mid\\
x^{(1)}& x^{(2)} & \dots & x^{(m)}\\
\mid & \mid & & \mid
\end{bmatrix}$,

$X \in R^{n_x \times m}$
<br/> <br/>
Also, $Z = [z_1,\dots z_m]$
<br/> <br/>
And, $W^T = [w_1, \dots w_{n_x}]$. Note that we just declare $W = w$
<br/> <br/>
So, $Z = W^TX + b$
<br/> <br/>
$A = \sigma(Z)$
<br/> <br/>
$dZ = A - Y$
<br/> <br/>
where $A = [a^{(1)}, \dots, a^{(m)}]$, $Y = [y^{(1)}, \dots, y^{(m)}]$
<br/> <br/>
$db = \frac{1}{m} sum(dZ)$
<br/> <br/>
$dw = \frac{1}{m}Xdz^{T}$
<br/> <br/>
$w = w - \alpha dw$
<br/> <br/>
$b = b - \alpha db$
<h2>Multi layered neural networks</h2>
First layer, i.e. input layer: provides the input $x$, also called $a^{[0]}$
<br/> <br/>
<img src="./neural-net-2.png"/>
<br/> <br/>
Second layer computes $z^{[1]} = \begin{bmatrix} z^{[1]}_1\\z^{[1]}_2\\\vdots\\z^{[1]}_k\end{bmatrix}$
$=\begin{bmatrix} w^{[1]T}_1 * a^{[0]} + b^{[1]}_1\\ w^{[1]T}_2 * a^{[0]} + b^{[1]}_2\\ \vdots\\w^{[1]T}_k * a^{[0]} + b^{[1]}_k \end{bmatrix}$
<br/> <br/>
and then, $a^{[1]}_i = \sigma(z^{[1]}_i)$
<br/> <br/>
Superscript $[1]$ denotes the first layer of neural network and subscript $i$ denotes the $i^{th}$ element
of the first layer.
<br/> <br/>
Now that you have $a^{[1]}$ ready, can compute $a^{[2]}$ applying similar logic.
<br/> <br/>
<h3>Vectorizing computation of $z^{[1]}$ etc.</h3>
We stack various $w^{[i]T}$'s below each other and call it $W^{[1]}$, and then call
<br/> <br/>
$z^{[1]} = W^{[1]} * x + b^{[1]}$
<br/> <br/>
or
<br/> <br/>
$z^{[1]} = W^{[1]} * a^{[0]} + b^{[1]}$
<br/> <br/>
and $a^{[1]} = \sigma(z^{[1]})$
<h3>Vectorizing across multiple training examples</h3>
Now vectorizing across multiple training examples is also not too hard:
<br/> <br/>
As usual, various columns denote various training examples
$Z^{[1]} = W^{[1]} * A + b^{[1]}$
<h3>Various activation functions</h3>
Activation functions are the source of non linearity.
<ol>
<li>sigmoid: $\frac{1}{1 + e^{-z}}$</li>
<li>tanh: $\frac{1 - e^{-z}}{ 1 + e^z}$</li>
<li>RelU</li>
<li>LeakyRelu</li>
</ol>
Their derivatives:
<li>sigmoid: $g(z) (1 - g(z)$</li>
<li>tanh: $1 - tan^2(z)$ </li>
<h2>Gradient descent implementation</h2>
Parameters are $w^{[1]}$, $b^{[1]}$, $w^{[2]}$, $b^{[2]}$
<br/><br/>
First layer has $n_x = n^{[0]}$ units, second layer has $n^{[1]}$
units and last layer (for two layer neural net) has $n^{[2]} = 1$
units.
<br/><br/>
$w^{[1]}$ is $n^{[1]}\times n^{[0]}$ matrix.
<br/>
$w^{[2]}$ is $n^{[2]}\times n^{[1]}$ matrix.
<br/>
$b^{[1]}$ is $n^{[1]}\times 1$ matrix.
<br/>
$b^{[2]}$ is $n^{[2]}\times 1$ matrix.
<br/>
$J(w^{[1]}, w^{[2]}, b^{[1]}, b^{[2]}) = \frac{1}{m}\sum_{i=1}^mL(\hat{y}, y)$
<br/>
Repeat {
<br/>
   &nbsp;&nbsp;&nbsp;Compute $\hat{y^{(1)}}, \hat{y^{(2)}}, \hat{y^{(m)}}$
   &nbsp;&nbsp;&nbsp;<br/>
   &nbsp;&nbsp;&nbsp;$dw^{[1]} = \frac{dJ}{dw^{[1]}}$, $dw^{[2]} = \frac{dJ}{dw^{[2]}}$
   &nbsp;&nbsp;&nbsp;<br/>
   &nbsp;&nbsp;&nbsp;$db^{[1]} = \frac{dJ}{db^{[1]}}$, $db^{[2]} = \frac{dJ}{db^{[2]}}$
   &nbsp;&nbsp;&nbsp;<br/>
   &nbsp;&nbsp;&nbsp;$w^{[1]} = w^{[1]} - \alpha dw^{[1]}$
   &nbsp;&nbsp;&nbsp;<br/>
   &nbsp;&nbsp;&nbsp;and so on
   &nbsp;&nbsp;&nbsp;<br/>
}
<br/>
where
<br/>
$dz^{[2]} = a^{[2]} - y$
<br/>
$dw^{[2]} = dz^{[2]} a^{[1]T}$
<br/>
$db^{[2]} = dz^{[2]}$
<br/>
$dz^{[1]} = W^{[2]T}dz^{[2]}  * g^{[1]'}z^{[1]}$
<br/>
$dw^{[1]} = dz^{[1]} x^{T}$
<br/>
$db^{[1]} = dz^{[1]}$
<br/>
and thus
<br/>
$dZ^{[2]} = A^{[2]} - Y$
<br/>
$dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]T}$
<br/>
$db^{[2]} = \frac{1}{m}np.sum(dZ^{[2]},axis=1,keepdims=True)$
<br/>
$dZ^{[1]} = W^{[2]T}dZ^{[2]}  * g^{[1]'}Z^{[1]}$
<br/>
$dW^{[1]} = dZ^{[1]} X^{T}$
<br/>
$db^{[1]} = \frac{1}{m}np.sum(dZ^{[1]},axis=1,keepdims=True)$
</body>
</html>
