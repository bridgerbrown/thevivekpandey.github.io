Learning rate is most important

momentum term: 0.9 is good size
mini batch size
number of hidden units

number of layers
learning rate decay

If you use adam algorithm, default parametersof adam algorithm are good enough


How do you explore hyper parameter space?

If there were few parameters, you could explore points in grid like so.
<insert image>

But with many parameters, choose points at random, this way you get many 
values of all the parameters.
<insert image>

Coarse to fine:
You search in the zone where you are finding lower cost values

Choosing right scale
---------------------
If the range to search is very large, logarithmic sampling is better
than linear scale. Eg. is alpha can be between 0.0001 and 1 and you
have to choose 5 values, you should choose 0.0001, 0.001, 0.01, 0.1, 1

If the range is small but increasingly close to a number, logarithmic
scale is used in a different way. Eg if can be between 
0.9 and 1 you should 0.9, 0.99, 0.999, 0.9999 etc.

Pandas vs Caviar
----------------
Pandas: babysit one model, change the parameters periodically and experiment.
This is done if you have less computation resource.s

Caviar: Train many models in parallel. Take the best.

Batch normalization
-------------------
Normalizing inputs can speed up learning
You normalize z across training examples so that in a deep neural net, the weights of layers are learnt
faster.

Why batch normalization works
- Just like input normalization helps in faster learning so does hidden layer output normalization
- Ouput of deeply hidden layers becomes robust to changes in inputs

If you train your cat vs non-cat example on black cats only then it will not be able to
do well on white cats. black cat to white cat change is called "covariate shift"

Softmax
-------
