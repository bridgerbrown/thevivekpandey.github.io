<html>
<head>
  <title> Introduction </title>
  <link rel="stylesheet" type="text/css" href="../../style.css"/>
</head>
<body>
<h1>Introduction</h1>
<p>
These are exciting times to be a practitioner of Computer Science! 
</p>

<p>
For past several decades, Computer Science has been changing our lives
at an unprecedented rate, so much so that we call the phenomena as
information revolution, at par with agricultural revolution and 
industrial revolution.
</p>

<h2>It's all about information</h2>
<p>
What makes computers so powerful? What makes computers connected to the
internet even more powerful? Calculators to can calculate, but they
don't have as dramatic impact on our lives. Data transfer is possible
by means other than internet, but that too does not impact our lives
so dramatically as the internet.
</p>

<p>
Information ingestion, manipulation and storage are the requirements for
survival of any living being. Consider a bird for example. It must take
in information via vision and sound, manipulate it (to locate potential
food items, or its nest), and also store the information (for example,
it needs to remember the location of its nest)
</p>

<p>
Various of our inventions help in information ingestion (like camera,
television), information manipulation (like calculators) and information
storage (like books). Computer systems, however, dramatically increase
the capacity of ingestion, manipulation and storage. Furthermore, 
computer systems continue to increase these three aspects continuously,
thus enabling newer problems that can be solved using computers.
</p>

<p>
First let's look at information ingestion rates per online user.
<table border="1">
<tr><td>Year</td><td>Global Internet Traffic_1</td><td>Number of online users_2</td><td>Data transfer per day per online user (col1 / col2)</tr>
<tr><td>1992</td><td>100 GB per day</td>       <td>4 million</td>   <td>25MB</td></tr>
<tr><td>1997</td><td>100 GB per hour</td>      <td>70 million</td>  <td>34MB</td></tr>
<tr><td>2002</td><td>100 GB per second</td>    <td>600 million</td> <td>14GB</td></tr>
<tr><td>2007</td><td>2,000 GB per second</td>  <td>1200 million</td><td>144GB</td></tr>
<tr><td>2016</td><td>26,600 GB per second</td> <td>3600 million</td><td>638GB</td></tr>
<tr><td>2021</td><td>105,800 GB per second</td><td>5400 million</td><td>1.7TB</td></tr>
</table>
</p>

1. https://www.cisco.com/c/en/us/solutions/collateral/service-provider/visual-networking-index-vni/vni-hyperconnectivity-wp.html
<p>
So, we are consuming close to 1TB per online user per day, and the number continues to rise.
Just how big is this number? Let's compare it to data ingestion done by a human
without using any technology. Most of the data ingestion for humans is via eyes
and ears, which are roughly equivalent to a video, which keeps on playing for
us for 16 hours a day. A high quality 1 hour video consumes around 40GB of space.
Thus, over the course of a day, an average human consumers around 640GB worth of
information. Note that not all information is the same: information coming via
the reading of a book has a much higher information density compared to gazing
in a field, since the book contains the distilled knowledge of the author.
However, such a comparison of total information ingested per human gives us some
idea of how advanced our technology is compared to our biology.
</p>

<p>
Above analysis seems to suggest that per capita computer data transfer has reached
the level of our eyes and ears just recently. Since data transfer is on the rise,
we can expect computers to augment the humans ever more, at least by this metric.
</p>
<p>
Computer Science
has been transforming the way we live at an increasingly rapid pace since 1960s.
First, there has been the revolution of micro processors. From a 4 bit, 740KHz
microprocessor in 1971, to 64, 2 GHz, multi core processors today, the speed
of microprocessors has relentless increased, along with accompanying increase
in capacity and decrease in cost of RAM and disks. 
</p>

<p>
Secondly, there has been advent of internet, with increasing bandwidths and decreasing
costs.
</p>

<p>
Above two phenomena at the hardware level led to creation of software enabling
a myriad number of use cases.
</p>

<p>
No party goes on forever and no field grows on for ever. 1920s and 1930s was the period 
when rapid advances were made in Quantum Physics. Indeed, in a popular photograph taken at 
1927 Solvay Conference, you can spot many physiciscs whose names are familiar to
any one who has taken a university level Physics course: Einstein, Schrodinger, Pauli, 
Heisenberg, Dirac, Neils Bohr, Max Plank, Marie Curie are all present in that photograph,
considered to be the photograph with the most intelligent picture ever taken.
</p>

<p>
However, as Quantum Mechanics matured, the rate of development of Physics slowed down
in 1940s and more so in coming decades. While Physics of course continues to progress,
and we periodically hear noises around Large Hadron Collider and God Particles, the 
rate of discovery is nowhere close to early 1900s.
</p>

<p>
Similar is the story of aviation. Wright brothers' first flight was in 1903, capping
the human flight attempts essentially since the dawn of humanity. First commercial
flight was in 1914, while first transatlantic flight was in 1919. Humans continued
to master how to fly, and by 1960s we were out in space, landing on the moon in 1969.
</p>

<p>
Looking at all this progress, people were speculating that we might soon be vacationing
on the moon and flying to Mars. However, the progress in aviation and space flight
got stalled: no human has yet landed on the Mars and flights to Moon are quite rare. Infact,
there have only been 6 manned missions to Moon!
</p>

<p>
Computer Science is one of the most exciting fields today, and the main engine
of technological advancement that we see today. The world is getting connected
very fast, and computers keeping better, enabling newer and newer applications.
Rapid advance in artificial intelligence are bringing up applications which
we have imagined only in fiction. While no one can be sure where this will
ultimately lead us to, the journey is surely exciting for those make this happen.
</p>

<h2>Why is writing software so complex</h2>

<p>
Writing any non trivial piece of software is hard. As the software becomes
more complex, the rate at which you can modify it becomes slower, and the
chances of introducing a bug in it become larger.
</p>

<p>Software engineers and managers are well aware of this, and thus, in any reasonable
sized company, there is an emphasis on testing. Large companies have large
Quality Assurance departments. In behemoths like Microsoft, strength of testing team
rivals the strengths of development teams: there is often 1 tester per developer. The code
is frozen several months before the release, and only bug fixes are allowed after
the freeze. This is so that the QA department can test the code thoroughly. This speaks
volumes about how we much confidence we have in the software that we write.
</p> 

<p>
The field of software engineering is not alone in being complex: any worthwhile human
enterprise is complex: it takes months or years to devise experiments for Large Hydron
Collider, and peer review for path breaking papers in Mathematics takes several months
too. However, there is something unique about software engineering: at each stage we
spend large amount of effort in debugging the software, and yet the quality of the
software that comes out is often less than desirable:
</p>

<img src = "../BSOD_Windows_8.png" />

<p>
Why is writing software so complex? Can't we be a little more careful and write correct code? The complexity
of software arises from two fundamental reasons.
</p>

<p>
First, humans have a capability to hold a limited amount
of state in their brains. In his paper "The Magical Number Seven, Plus or Minus Two: Some Limits 
on Our Capacity for Processing Information",
George A. Miller, a cognitive psychologist at Princeton University, posits that human beings
are able to hold between five and nine pieces of information at a time. Give them more than that
and things start falling apart.
</p>

<p>
In the game of chess, this is known as blunder. Novice players often see that their particular piece,
say Queen, is under attack, and they are fully aware that they need to do something about it. However,
when they start exploring various possible lines, they sometime tend to forget that their Queen is
under attack, and make a silly move at the end. This is because we have a limited capacity to hold
state in our head.
</p>

<p>
Computers are giant state machines. The content on the hard disk, the memory, various caches and the 
processor registers constitutes the state, and various instructions are rules to change the state
from one to another.
</p>

<p>
To manipulate such a giant state machine is very hard, and thus, higher level programming languages
provide us with a simple, abstracted out state machine. Variables and objects are the states and
statements operate on those states.
</p>

<p>
It remains a state machine nonetheless and human capacity to hold a non trivial state information
is just not there.
</p>

<p>
Were computer programming 
</p>

<h2>Handling software complexity</h2>
<p>
So, software development is inherently complex. It is not that individual programmars are lazy 
or incomptent: software development is inherently hard for humans. Were it a stateless activity,
like typing, we could become good at it. Were the number of states fewer, like in driving, we
could have a much better accuracy. Humans are much better at driving compared to coding: 

<img src="../deaths-per-mile.png" width="800px"/>

There are only about 20 deaths per billion miles driven, while there are "15-50 errors per 1000 lines of
delivered code": https://softwareengineering.stackexchange.com/questions/185660/is-the-average-number-of-bugs-per-loc-the-same-for-different-programming-languag

And in case you are getting complacent that driving safely is more important than writing bug
free code, be aware that computer code is handling medical equipments, driving cars and controlling satellites.
It is a wonder that the Earth is spinning on its axis despite so much bug ridden software floating around.
</p>

<p>
So, the question is: given that computer programming is not a natural activity for us humans, what do 
we do to still deliver a high quality software?
</p>

<p>
There is a three pronged strategy that software developers need to follow to deliver high quality software:
<ol>
<li>Work tirelessly to reduce the complexity of the software</li>
<li>Test the software in before you deploy it </li>
<li>Monitor the software for probelms after you deploy it</li>
</ol>
</p>

<p>
First, since software development is bug prone because it is complex, we strive to reduce the complexity
of the software. Further since software is complex because of large state involved, we strive to reduce
the number of states that we have to deal with.
</p>

<p>
The general principle of reducing the number of states that we deal with can be applied at various levels,
from coding style to architecting the systems. 
</p> 
<p>
While coding, you should strive to have fewer variables
in the function (each variable is a state), name each variable suitable (to enable better association
in our brains). Use fewer conditions, because code inside the conditionals has more state information
to be tracked compared to the code outside the conditionals. Nested ifs are even worse. We will consider
several techniques to reduce complexity in one of the upcoming chapters.
</p>

<p>
At architectural level too, we try to reduce complexity by deduplicating information between components,
and reusing code across components.
</p>

<p>
Secondly, however hard we may try, software that we write will always contain problems, ranging from silly
mistakes to lack of understanding about the system should work. As we write our code, we also write test
cases to protect against erroneous future modifications. Whenever we find a bug in the production, we add
a test case so as to avoid the appearance of the same bug in future.
</p>

<p>
Lastly, even after thorough testing may not reveal problems related to the lack of foresight of developers
about how the code will be actually used. The input provided to the program may be in a different
format than assumed by the developers, network requests may time out or provide unreasonable responses
and so on. Thus, it is important that we monitor the behavior of the code in production. 
</p>
</body>
</html>
