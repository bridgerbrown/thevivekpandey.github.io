<html>
<head>
  <title>On Performance Improvement</title>
  <link rel="stylesheet" type="text/css" href="../../style.css"/>
</head>
<body>
<h2>On performance improvement</h2>

<p>
Everyone wants superior performance for their product or service. However, unless you are methodical
about it, it is easy to spend a lot of time optimizing your code and systems, with little actual
improvement to show for the efforts. This chapter discuss some of aspects to consider when trying
to improve the performance of the systems.
</p>
<h3>Do you need superior performance?</h3>
<p>
While having a good performance is, well, good, before you take up an optimization attempt, you should
take a step back and think if you need better performance. Any time put in improving performance is
time put in doing some other activity, and thus performnace improvement efforts need to justify
themsleves relative to other activities that could be done.
</p>

<p>
This sounds obvious, but many times, engineers are driven by their insticts of aesthetics and
problem solving, rather than pragmatism. The thrill of improving software may outweight the 
pragmatic consideration whether their improvemnt is required at all.
</p>

<p>
I was once walking and on the sidewalk and saw the pedestrian signal turn green at a distance.
I committed to myself that I am going to cross the road before the signal turned red. I 
started running, and just managed to cross the road before the signal turned red. Only after
this I realized that my destination was on the other side of the signal and I never needed
to cross the road. I waited for another green signal to cross the road back. Indeed, sometimes,
overcoming the challenges becomes more important than practical considerations.
</p>

<p>
Does it really matter if your nightly job takes 30 min instead of 1 hour? Does it matter
if you reduce the memory consumption of program from 100 GB to 90GB? Would your users
appreciate if the latency of requests goes down from 20ms to 19.8ms? In many situations,
the answer is no, and you should move on to the next problem, which hopefully, matters.
</p>

<h3>Amadahl's / Gustafson's laws</h3>
<p>
Another factor to keep in mind while working on performance improvement is Amadahl's law.
Amadahl's law, reworded as Gustafson's law
--footnote--
Amadah's law and Gustafson's law are very simple facts. These guys were lucky that they
coined their laws which became so famous.  Gene Amadahl was an exciting fellow, who kept 
on founding companies till his sixties, and led a remarkable life.

states that overall speedup of a program is dependent not only on the speed up by which
you increase the latency of some portion of the program, but also on the portion of the
program which you do not improve.
</p>

<p>
Consider an example. Suppose a program's execution time is 100s. Suppose its execution can be 
divided into two parts: A and B. Part A takes 20s and Part B takes 80s. Now, if you speed up
part A by 90%, such that Part A takes only 2s, overall the program will take 82s, and thus
there is a speed up of 12% in overall program execution time. However, if you speed up 
Part B by a more moderate 50%, Part B takes 40s, leading to an overall execution time of 60s,
implying a speedup of 40%.
</p>

<p>
While you may see green pastures of optimization opportunities in parts of your system, do not
jump to work on them unless the overall impact of those improvements justifies the efforts.
</p>

<p>
Say you are writing an HTTP server. Say it takes an average of 10ms for your server to respond to
the request. You have figure out a way to reduce it by 20%, so that it will take only 8ms. 
10ms vs 8ms sounds like a handsome improvement, and your end users will appreciate a faster
website, won't they?
</p>

<p>
Well, not so fast. You need to consider that from end user's persepective, there is network
latency too: San Francisco to New York network latency is like is ~50ms, cross continent
network latency is even higher. So, you are reducing user preceived latency from 60ms to 58ms,
which is not that impressive.
</p>

<p>
Note that if you are doing 10ms to 8ms change with code optimization, without introducing
extra hardware (e.g., for caching), you may still justify your attempts if there is significant
infrastructure cost reduction. You then need to check how much money you end up saving.
</p>

<p>
Perhaps, while reviewing the code you found that there is a linear search happening on an array,
and using hash map can make search O(1). But before you implement improvement, consider the
size of the array, and how many times that search is done. Will you be improving the 
performance sizeably?
</p>

<p>
Perhaps you found that a datbase table is being queried on a certain column and there is
no index on that column. Before you create the index, consider how much does that buy you.
Is the table size large enough for the index to matter at all?
</p>

<h3>Measurement driven optimization</h3>
<p>
"Premature optimization is the root of all evil" said Edgar Dijkstra. For him to come up with
the quote, he himself would have wasted some time in premature optimization, or else seen
others wasting their time. Indeed, premature optimization is an urge that is to be resisted
continually.
</p>

<p>
So, what is full term (as opposed to premature) optimization? It is an optimization driven by
measurement. You measure, make an educated guess about the efficacy of the improvement
that you are undertaking, do the improvement, and measure again. Then you repeat the process.
</p>

<p>
The measurement consists of two parts: the metric and the profile.
</p>

The metric is usually a single
number representing the performance of an entire sub system. It could be running time of a program. 
For an HTTP server, this could
be response time. For a build system, this would be time to build a fresh build. In a throughput
oriented system, it could be how many requests per second a server can support before
CPU utilization of the server exceeds 70%. For a chat server, it could be the number of
concurrent connections supported before the response times exceed, say 10ms. For a frontend app
it could be page load time. For a caching server, it could be the hit rate.
</p>

<p>
Sometimes, the metrics could be at high level: how many batch jobs are failing per day? How many
open bug reports are there?
</p>

<p>
You often need to aggregate individual numbers to come up with a single metrics. 
In statistics, they are called "measures of central tendency". Typical mesures
of central tendecy are mean, median and mode.
For example,
while calculating the overall latency of all the requests sent to an HTTP server over a day,
you may take the average of individual latencies. While calculating aggregate metrics, while
average sounds most natural, this may not always be the case: there are instances when
90%ile value may be better, or maximum may be better.
</p>

<p>
For metrics which do not affect the end user directly, arithmetic mean is usually a good idea.
Let's say a program runs once every hour, and you want to have a metric for its performance.
An arithmetic mean of the 24 run times over the day would be a good choice. Similarly, if you
are tracking CPU utilization of a machine and taking a measurement every 5 minutes (like
you see in AWS CPU utilization dashboard), taking an average over a desired period is a good
idea. For example, if a job takes
t1, t2, ..., t24 units of time, you can take (t1 + t2 + ... + t24) / 24 as a respresentative 
running time of the program.
</p>

<p>
For some metrics, an average is an unsatisfactory measure. Let's say that you have a machine
with 4GB memory, and you run a memory heavy program on it. You measure the memory utilization
of the program at important points in your program (when you expect the program to take locally
maximum amount of memory). Say you measure the memory at 4 points in your program and find it
to be 200MB, 600MB, 800MB and 3.2GB. Then the average (200 + 600 + 800 + 3200) / 4 = 1200MB 
= 1.2GB is not a good measure of aggregate memory utilization. You should watch for max value,
since if this max gets close to 4GB, you have a problem in your hand: you either need to
move to a bigger memory machine, or optimize your program to make it use a lesser amount of
memory.
</p>

<p>
Similarly, some times, minimum rather than maximum can be appropriate value to use. For
instance, if you are tracking free memory in the system, you need to know how close to 0
do you reach, and hence tracking the min free memory of a time period is appropriate
</p>

<p>
Suppose you have written a backend to generate some report based on
user input. Depending on the user input, report generation may range from being extremely
fast to extremely slow. You want a single number to denote the aggregate performance. 
In such a situaation, neither arithemetic mean nor maximum is a suitable measure
of central tendency. Say, there are 100 requests, 99 of which took 1s while 1 request
took 1000s. Now the average response time is (99 * 1 + 1000) / 100 = 10.99s, which is
not representative since most requests are taking just 1s. Maximum is even worse.
A good measure in this case is to take some percentile value, like 95 percentile, 
or 99 percentile. If 95 percentile response time is t, it means that only 5 perecent
of the requests takes more than time t. So, you have an idea how many users are being
pained by poor performance. In the above case, 95 perecentile response time would be 1s,
which is a good measure of central tendency.
</p>

<p>
Percentile values are good to use in cases where a few abnormally large values may
skew the aggregates like mean and cause it to become unrealistic. Remember that
when Bill Gates walks in a room, the average net worth of a person in that room
is north of a billion dollars.
</p>

<h3>Some ideas to improve performance</h3>
<p>
While performnace improvements are quite application dependent, there are some general
patterns which are useful to know.
</p>

<p>
<b>Caching</b>
Caching is perhaps the most important idea for performance improvements. Caching
refers to the practice of storing the result of some computation or operation in a
quickly accessible location so that next access is faster.
</p>

<p>
Idea of caching is used pervasively in computer systems:
<ul>
<li>
Processors use memory hierarchy. L1, L2, L3 caches are on chip (or slightly off chip)
caches with store data from memory. They are fast compared to memory. Processor
utilizes the fact that most of the time you work on a small subset of data stored in
the memory, and by storing this subset in the cache, program can be run much faster.
</li>

<li>
All databases use in memory caches, in which they store data fetched from the hard disk.
</li>

<li>
File system too implements a file sytem cache. The idea is that if a part of the file is
accessed now, it is also likely to be accessed again, may be by a different program.
</li>

<li>
Virtual Machines, which translate code on run time, cache the translation to be used
at a later point of time. An example would be Java runtime environment.
</li>
<li>
Content Delivery Networks (CDN) cache static web pages across geographical nodes so as to
server static content to respective users from a location close to the end user. You can also
use a CDN to speed up the delivery of static content. Amazon's CDN, called Amazon CloudFront,
is easy to use.
</li>

<li>
The popular memoisation technique in dynamic programming algorithms is a manifestation
of caching.
</li>

<li>
Many websites cache user data so that user login is fast. In memory data stores like Redis
and Aerospike are good caching servers for such purposes.
</li>

<li>
Web browsers cache the pages that you view, so that the pages load faster next time. A web
server can set directives about how long a page can be cached.
</li>

<li>
As seen in chapter x.y.z, you can put a caching server in front of database, so that idempotent
write queries to databases are reduced, thus reduce the load on the database servers.
</li>

</ul>
</p>
<p>
At times, you can couple the idea of incremental computation with caching. You may be able
to perform a computation by using the cache and then doing an incremental calculations. 
If you want to find the set of users who have used an app in last 7 days, and you have
cached the user list on a daily basis, you pick last 6 points (that is caching) and find
out the users who have used the app today (that is incremental computation). [To be
precise, you should also consider an interval of time which was 7 days ago, that too
would be incremental computation]
</p>

<p>
<b>Precomputation</b>
Precomputation is another big idea for optimization. Instead of computing when the
need arises, we perform do the computation speculativel, and return the result immediately
when requested. This too is used across computer science.
</p>

<p>
<ul>
<li>
<p>
In processors, there is a "pipeline" which executes instructions one by one. When it encounters
an "if" instruction, it needs to take one of the two paths, depending on which way the "if"
condition turns out.
</p>
<p>
If for some reason, executing some instruction takes time (perhaps because they involve time
taking mulitplication or division, or a fetch from memory), processor continues to execute
next instructions, without waiting for previous instructions to finish (as long as input of
those instructions are ready). This is called out of order processing. 
If they encounter an "if" instruction while executing out of order, the processor makes
an educated guess (called branch prediction) about which way the branch will be resolved
and continues exeucting down that path. In case the guess turns out to be wrong, the processor
discards the results related to computation after the branch and restarts from that point.
Branch prediction and out of order execution are two of the chief mechanisms for processors
to get higher IPS (instructions per second) speed.
</p>
</li>
<p>
When a program requests the operating system to read a part of the file from hard disk to memory,
some file systems perform what is called "speculative prefetching". They fetch bytes of the
file from hard disk to memory in anticipation of requirements of the program. Speculative
prefetching is also performed by browsers in that they speculatively fetch the pages which
the expect the user to request.
</p>

<p>
Some databases implemente "materialized views", which is the result of a query computed periodically
and kept aside for quick query response time.
</p>
</ul>

<p>
Idea of precomputation may be applied in a myriad number of ways. At QGraph, we had a popular
use case where our customers segmented their user on the basis of city: for example they found
out all user the users whose city was, say, New Delhi or Bangalore. For each segmentation, querying
the database was time consuming, so we ran a nightly job which performed this segmentation.  When
a user requested the segmentation, we could fetch the precomputed results, and add to it data which
had since last night to output the final set of users.
</p>

<b>Batching</b>
<p>
Another idea to improve the performance of computer systems is batching, where instead of invoking
some service one by one, once for each request, you collect a bunch of requests and invoke the
service just once.
</p>

<p>
Batching too is all pervasive in computer engineering. Let's have a look at some popular use cases.
</p>

<ul>
<li>
In processor caches, there is a concept of cacheline. Cache line size can be anywhere between
64 bytes to 512 bytes. When a program accesses a memory location which is not present in the cache,
cache line size is the granularity at which data is fetched from the meory to the cache. So, while
you may be needing only 4 bytes, a 64 byte aligned 64 byte block will be fetched from the memory.
This is an example of a batched read, the idea being that if a program accesses certain memory location
now, it is likely to access nearby memory locations later.
</li>

<li>
In a similar way, data fetching from hard disk to main memory is also batched.
</li>

<li>
In databases, if, instead of doing lots of queries, each reading or writing one record, it is more
performant to do a few large queries, each reading or writing several records
</li>
</ul>

<b>Compression</b>
<p>Another optimization idea is compression. It can serve in two ways: Firstly you can store
compressed data instead of raw to save disk space. Secondly, you can compress data before transferring
it to reduce network bandwidth cost. Depending on how far the sender and receiver of the data are,
it may take smaller time to compress the file, send it over the network and uncompress it at the
receiver, rather than sending it uncompressed.
</p>

<ul>
Compression too is widely used in computer systems.
<li>
<p>
Databases often keep the contents compressed. Various database engines keep the data compressed.
You can turn on compression in InnoDB engine of MySQL, and compression is an important feature
for wired tiger storage enginer of MongoDB. Storing the data in compressed format is important
for those NoSQL databases. In MongoDB, if there is record per user, it will look like:

<pre>
{
   "firstName": "John"
   "lastName": "Doe"
   "city": "London"
   "profilePic": "http://www.example.com/johndoe.jpg"
}
</pre>
Each record contains the keys "firstName", "lastName" etc, which occupies a lot of space. Compression
helps alleviate the extra disk storage that happens due to this.
</p>

<p>
For OLAP workloads, columnar databases like Vertica are very popular. Once of the main featurs that
makes columnar databases so effective for OLAP queries is that columns can usually be compressed
very effectively.
</p>
</li>

<li>
Servers can compress the data before sending it to browsers. Browser uncompresses the received
data and then renders it. This reduces the overall page load latency.
</li>

<li>
Popular message queues like Kafka have an option where publisher compresses the data before putting
it to the queue and subscriber uncompresses the data on receving it. This leads to faster tranmission
of data from publisher to subscribers.
</li>

<li>
Of course, all popular image, video and sound formats like JPG, MOV or MP3 compress the files
for faster transmission.
</li>
</ul>

<b>Indexing</b>
<p>
Another big idea to improve performance is indexing. Indexes are everywhere in real world.
You find indexes in libraries, which are used to locate a particular book. You may say
who goes to library in today's networked world. Well, internet is powered by a giant index
called Domain Name Service!
</p>

<p>
File system is an index to the pieces of data lying around in the disk.
</p>

<p>
Creating suitable indexes for improved database performance is perhaps DB101, but its importance
cannot be overemphasized. Indexes are particularly useful for searching by columns which have
high specificity, i.e. given a column value, there are very few (ideally, only one) rows
matching the column value. However, for the cases where there is low specificity, indexes
are not that useful. For instance, each user may have a city, and you may wish to find
all the users who belong to a specific city. In such cases, I have found the idea of
precomputation (described earlier in this chapter) to be useful. At an abstract level,
precomputation is like an index, except that index is updated not in real time but
periodically.
</p>

<b>Asyncrhonous operation</b>
<p>
If you make a lot of requests to a server serially, you may consider firing the requests
asyncrhonously: you do not wait for a request to finish before you fire another request.
</p>

<p>
Popular langauges all support asyncrhonous operation. Since Javascript is asyncrhonous from 
ground up, framework like nodeJS is very easy to write asyncrhonous programs in. However,
I found it hard to control the fan out: the number of concurrent operations in progress.
I found Java to be an excellent language in which I have been practically able to write
non trivial asynchronous program. I have tried Python too, which support asyncrhonous
programs since Python 3.5, but had lesser success in writing non trivial systems in Python.
</p>

<b>Tradeoff accuracy for performance</b>
<p>
Many engineers have an unspoken principle that they try write 100% correct programs.
Like students who try to get 100% marks score low marks (because they get stuck
in some topic and cannot even cover the whole syllabus), they disregard the fact
that perfect is the enemy of good.
</p>

<p>
Often you can tradeoff a bit of accuracy for giant improvement in performance.
</p>

<p>
Consider the generation of a unique id. If you want to guarantee that the generated
id is unique, you have a few choices. You can initialized a counter to 0. Each time
an id is requested, you increment the counter and return the current value of counter.
However, this requires you to maintain the current value of counter. You need to
persist it too so that you do not lose the counter value if your program is restarted
or crashes. You also need to worry about parallelism: what it two clients simultaneously
request for an id, can they get teh same value of the id?
</p>

<p>
Alternatively, you can keep
storing generated ids in a database, and, on each request, generate a random id, and check
if it is already present. If yes, you store it in database and return. If no, you generated
another random id and repeat.
</p>

<p>
However, the simplest solution would be to just generate a random id and return it. Very
very likely it is going to be unique. Let's do some calculation. Say you are generating
64 bit integers as id. Then you generated one of 2^64 numbers, which is roughly 10^18. 
Suppose in the lifetime of your application, you will required to generate a billion
ids. That is 10^9. The chances that two ids will match are less than 1 in 10^1000, and hence
you should not worry about it. Infact, it is more likely that you may have an accident
today, or even worse, a meteor may strike the earth.
</p>

<p>
There is a class of algorithms, called randomized algorithms which you can deploy to
get solutions which are very good, but not perfect, and are simple to code in.
</p>
<h3>Performance can always be improved</h3>
<p>
Sometimes, after months or years of working, one may tend to get jaded. One may think
that no further performance improvement that matters is possible.  
</p>

<p>
In such scenarios, you should start questioning your assumptions. Perhaps, given your
assumptions, various incremental steps that could be taken, have been taken. But what
if we remove the assumptions: is it possible to do drastically better, from the point
of view of user? The important thing is not view your problem as technology problem,
but try to improve a user visible metrics. 
</p>

<p>
We had a program called "daily report", which used to run nightly and calculate some
reports for our clients. It started off by taking a few minutes, but gradually
started taking several hours. If it crashed in the night, we used to run in the
morning (after fixing the cause of the crash), but in that case it used to be a little
late, and our clients would complain about the unavailability of the report after
checking it out in the morning.
</p>

<p>
We worked on the report and after a lot of effort, optimized it so that it started
taking only a few minutes rather than a few hours. This improved our system signficantly:
if it crashed, running it will take only a few minutes, and thus the reports 
were available soon.
</p>

<p>
Once we got running time to a few minutes, there was not point in getting it down further.
It was very hard anyway. So, how could we improve the daily reports? Well, we could
have tried to make it realtime, rather than nightly. Giving visibility to the client
on a realtime basis is a vast improvement over giving it nightly. Perhaps we could have
put in a place a system of anomaly detection, where an unsual data point would be
caught and an alert thrown in (this is not a performance improvement though). The point
is, an enthusiastic engineer always tries to make his systems more and more useful
and responsive.
</p>

<h3>What was luxury yesterday becomes a necessity today</h3>
<p>
A final word on performance improvement: while your users will be initially thankful to 
the improvements you do, soon the new performance will be necessity, and expected.
</p>

<p>
My father grew up in a village, and while growing up, he did not have an idea that
hair should be combed. There was no concept of a comb in the village. Then the comb
got introduced, and it was initally used only by fashionable youth of the village.
Then slowly, people started using a comb when attending festivals or marriages.
Gradually, over tens of years, the comb became a necessity. If you do not comb
your hair, you are perceived to be unclean, unkempt and generally careless.
</p>

<p>
What was a fashion statement yesterday became common place today.
</p>

<p>
We built a segmentation engine, where our customer could put in some conditions and
find the users satifying the condition. For instance, he could try to find the
people who viewed a product in last 10 days. More the number of days he inputted,
more was the running time of program. We saw this use case and optimized for
this use case. We brought down the running time of the program from minutes to
seconds. As soon as the users accusomted to several minutes of running time
noticed their jobs being finished in seconds, they simply increased the duration
and the program continued to take several minutes.
</p>

<p>
Were our efforts useless? No, because were delivering more value to the clients.
</p>

<p>
Scaling of expectations happens not only with humans, but machines too. To improve
the scalability of our data ingestion, we put several caches in the system. This
helped us scale with increasing number of requests without increasing infrastructure
cost signficantly. However, now the caches become indispensable. If they go down
for whatever reasons, whole data ingestion pipeline gets clogged. Initially the
caches were a luxury, but with time, they become an integral part of the system.
</p>

</body>
</html>
