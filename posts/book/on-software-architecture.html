<html>
<head>
  <title> On Software Architecture</title>
  <link rel="stylesheet" type="text/css" href="../style.css"/>
</head>
<body>
<h2>On Software Architecture</h2>
<p>
This chapter deals with miscellaneous topics on software architecture.
</p>
<h3>Don't theorize, do it, and then improve</h3>
<p>
There is a story which I have read from more than one sources:

The ceramics teacher announced on opening day that he was dividing the class into two groups. 
All those on the left side of the studio, he said, would be graded solely on the quantity of 
work they produced, all those on the right solely on its quality. His procedure was simple: 
on the final day of class he would bring in his bathroom scales and weigh the work of the 
"quantity" group: fifty pound of pots rated an "A", forty pounds a "B", and so on. Those being 
graded on "quality", however, needed to produce only one pot - albeit a perfect one - to get an "A".

Well, came grading time and a curious fact emerged: the works of highest quality were all produced 
by the group being graded for quantity. It seems that while the "quantity" group was busily churning 
out piles of work - and learning from their mistakes - the "quality" group had sat theorizing about 
perfection, and in the end had little more to show for their efforts than grandiose theories and a 
pile of dead clay.
</p>

<p>
Similarly, computer architecture is learnt only by practice. If you don't know how to architect
elegantly or correct, do not delay your project to think too deeply about architechure. Just do things
that seem logical, and be ready to change when the moment of truth strikes. You learn only
by experience.
</p>

<p>
I have two stories first hand, one when a system was architected badly and other where it was
architected well.
</p>

<p>
The first story relates to when we writing a realtime bidding engine. A realtime bidding enginer
receives a bid request from the supply side provider to bid on an ad slot. As a part of
the request, the bidding engine receives various properties of the ad slot (like the
website on which the ad is to be shown, the size of the slot, the location of the slot) and
that of the user (location of the user, his interests, etc). Based on all these parameters,
the bidding engine needs to decide how much money it is willing to pay for a particular
ad.
</p>

<p>
The product management team
gave the requirement that there will be a "base bid",
and then according to several rules, base bid will be multiplied by some factors. So,
perhaps if the user was in California, base bid will be multiplied by 3, while if he was
in Baghdad, it would be multipled by 0.3. We will arrive at final bid value after several
multiplications.
</p>

<p>
The engineer building the bidding engine was very enthusiastic. 
He saw that the sepcifications can modeled by a list of "conditions". Each condition needs
to be evaluated in one of the buckets. For example, the condition would be "country of the
user" and various buckets would be "India", "USA", "France", "Others". Or else condition
would be "Income group of the user", and various buckets would be "HNI, Affluence, middle class,
poor"
</p>

<p>
Our engineer decided to model conditions in a class called "Condition". He realized that there
could be several types of conditions: in some conditions, each bucket was a range of
values (e.g. for age, buckets could be: less than 18 years, between 18 and 25 years, between 25
and 45 years, more than 45 years). Or they could be discrete valued (like gender being male
or female). Or they could be ordered string match (A user falls in the first bucket that he matches)
He decided to have "Condition" as an abstract super class and several specific condition
classes inherit from it.
</p>

<p>
So far so good. Above abstraction faithfully models the requirements. But being enthusiastic and
bright, our engineer tried to foresee what the product team would demand in future. The idea
was to code such that upcoming requirements are also coded up and require no futher changes
to the code.
</p>

<p>
He decided that in the future, each condition may have sub conditions. In other words, some
conditions would be applicable subject to some other conditons falling in specific bucket.
Perhaps we would apply age filter only if the user was from United States.
</p>

<p>
To model this hypotetical need, he decided that each condition class will contain references
to an array of array of subconditions. There was one array of sub conditions corresponding
to each bucket of the condition. This complicated the whole code significantly and increased
the development time.
</p>

<p>
Our engineer remained for around 1 year after coding this up, and there was never a need
for sub conditions. When new engineers were required to understand the component, he
admitted that it was an overengineered peace of software and the newer engineers never
needed to learn how the sub-conditions worked. When the original engineer moved out,
there was no one in the company who understood sub-conditions.
</p>

<p>
A few years later, the code was migrated to Java (the original code was in PHP), and
sub-conditions were not required to be migrated.
</p>

<p>
Different requirements from bidding engine did arise. However, they were along the lines
which the original engineer had though. They were completely different and were coded up
as they arose.
</p>

<p>
The second story relates to the case when we needed to build a data ingestion system.
The problem statement was simple: we had to build an HTTP server which received data
and stored it in relevant database and relevant table in MongoDB.
</p>

<p>
We were just starting out and had no visibility into scaling issues that may arise in
future. We did not try to predict them either. We wrote a simple server in Django
REST framework, which connected to MongoDB on each request and fired a query to
store the data.
</p>

<p>
We saw that in times of peak load, data ingestion was not working properly: it was
taking too long to connect to MongoDB and requests were timing out. This was leading
to data loss. We realized that we needed to have a message queue between our Django
server and mongo db. A message queue can give us much higher throughput while writing
data because it is the only thing that it does. We decided to use Kafka as the message
queue. Now Django server writes to Kafka and there is a kafka consumer which inserts
data to MongoDB. Now, at the time of peak loads, it does take a few minutes for data
to get inserted to mongo, but at least there is no data loss.
</p>

<p>
While looking at mongo logs, we saw that connection to mongodb was being repeatedly
opened and closed by kafka consumer. On some reading on internet we came to know about
connection pooling. We realized we needed to have a connection pool rather than open
a new connection per request. We implemented this, and the CPU utilization of our
mongo db servers went down a little.
</p>

<p>
Things again went fine for a few months, when we started facing a fresh problem. MongoDB
was not only ingesting the information, but other systems fired MongoDB queries
for other purposes.
</p>

<p>
Again, we realized that bulk operations to mongo were more performance that individual
operations. We wrote a module, called mongo query aggregator, which provide similar
interface to other mongo drivers, but which aggregated the write queries and fired
them to mongo either when threshold number of queries were collected or when a threshold
amount of tasks (few hundres of milliseconds) had elapsed. This again reduced CPU
utilization of mongo and the system became reponsive again.
</p>

<p>
A few months later, as the ingestion rate and query rate increased again we started
having similar problems. Incidence of such problems rose from very infrequent (once 
four weeks) to mildly frequent. (once a week). At the time of large read queries,
the ingestion system would be too slow and it would take a lot of time for data
to reach from kafka to MongoDB.
</p>

<p>
We realized that a lot of data that we were getting was repeated data. For a given
user, we got his profile information as many times as he did some action. The profile
information that we got contained information like the name, email or the city
of the user. This information tends to change very slowly, but we wrote it to MongoDB
repeatedly. To reduce the query rate to MongoDB, we put an aerospike in front of
MongoDB. As we wrote some information to MongoDB, we also wrote it to aerospike. However,
we writing an info to MongoDB, we checked in aerospike if the same information
was present already. If yes, we did not bother to write the information in MongoDB.
This optimization reduced the MongoDB queries by around 60%.
</p>

<p>
Along the way we also realized that not all data is equal. For some types of data
it was important to get it to MongoDB as soon as possible. Other type of data could wait.
Thus, we started from putting data in two different queues from Django framework. When
there was delay in gettin data to Mongo, we could shutdown the consumption of lower
priority queue, thus ensuring that high priority data reaches mongo on time. Once the
peak traffic was over, we would start consuming the lower priority queue.
</p>

<p>
As the traffic increased, we also realized the django was not most efficient
framework to be used. The number of servers employed were increasing with increasing
load and we believed latencies could we lower. We migrated this system to Java
Jetty and brought down our infrastructure cost as well as reduced the latencies.
</p>

<p>
While our server was written in Django, nginx was the front facing server which routed
request to django via unicorn. When we shifted to Jetty, nginx continued to be front
facing server. However, we realized that we no longer needed nginx, since jetty could
serve well as front facing server. Thus, we removed nginx from our system.
</p>

<p>
Data ingestion system began with a simple 30 line django code, and by now it has
expaneded to several thousands of lines code, with multiple systems running on 
various machines. When we began we did not know how the system will evolve, and
we did not try to predict the problems that would happen. However, we kept our
eyes and ears open and solved the problem as they came.
</p>

<p>
The key is to be continually evaluating the systems for current performance bottlenecks
and how they could be resolved. You need to know what the current limitations of
systems and how they are currently impacting your uses. Then you take care to solve
the problems before they become too severe.
</p>

<h4>Theorization ad infinitum</h4>
<p>
Some fields are just so ripe for theorization that one can keep theorizing indefinitely
and not get started on real work.
</p>

<p>
Consider the problem of time series forecasting. Suppose you have a stream of numbers 
coming in, which represent some real world metrics, like CPU utilization on minute basis, or sales
of a product on daily basis. Your job is to make a component to predict the next value
of the metric.
</p>

<p>
This simply stated problem is such a ripe problem for theorization that you can spend
days or weeks or months just surveying literature. From ARMA models, to ARIMA models, 
to STL model, to incorporating seasonality, you can read several books and not
get started on real coding.
</p>

<p>
In such cases, you need to limit your study time, and start building soemthing so that
you can get some real world experience, and only bookish knowledge.
</p>

<h3>Soft on external components, hard on internal components</h3>
<p>
In any non trivial system, there are various components interacting with each other
in various combinations. Invariably, some of these components will misbehave. How
should other components respond to a misbheaving component?
</p>

<p>
Your system's components also interact with the world outside: perhaps you have
provided an API which your customers can use. Your customer code can awry and
starting sending you requests in an inappropriate format. What should you do then?
</p>

<p>
We can handle misbehaving components in two ways: one is that we are forgiving about
the input from the other component and try to do as good
a job as possible. A web browser is a great example of this: even if you give it
a malformatted HTML, it won't just refuse to render the web page. It will render
in the best possible way possible given the invalid document that you gave.
It is like a friend who tries to help you out despite your shortcomings.
</p>

<p>
On the other hand, we can be merciless: unless other components talk to us in
a rigorously predefined language, we will refuse to function. We will crash
or complain. A typical example of such an approach is compiler. Even if you
miss a single semi colon, it will refuse to compile the program.
</p>

<p>
Examples of browser and compiler are two ends of a complete spectrum of the
strategies that you can use to deal with incorrect inputs. The "right" approach
depends on your use case, and it is important to get it right. Otherwise
you may end up with a language like PHP where
</p>

<pre>
"b" + "a" = "ba"
but 
"10" + "a" = 10
and
"10" + 10 = 20
</pre>
<p>
causing all sorts of confusions.
</p>

<p>
My strategy is to have internal components be harsh on each other: you decided on
a data interachange format, and if the producer of the data does not follow the
format, the consumer crashes. The consumer does not try to cover for the bugs
in the producer.
</p>

<p>
For example, in a typical use case, the UI takes the user input and populates some
data in database in some predefined format. Some backend task needs to operate
on this data and perform some activity. Now, what if there is a bug in the UI and
the UI outputs the data in slightly incorrect format. Perhaps the UI does not
provide the value of a required field. Should the backend assume some resonable
default? Assuming something reasonable might ensure that the system will at least
work. The answer is no. Between internal components, we do not forgive each other
for mistakes. If someone makes a mistake, associated functionality comes to a halt.
</p>

<p>
The advantage of this approach is that if there is a bug in some component it comes to
light at an early stage, enabling the engineers to resolve it. If other components
start covering for it, the bug may remain uncovered for a long time.
</p>

<p>
Languages like Python allow for default values of parameters. Thus, a function definition
may be
</p>

<pre>
def my_function(a, b=v1, c=v2):
    pass
</pre>

Here b and c are optional parameters whose default values are v1 and v2 respectively. These values
will be used if you do not provide a value for these parameters.
Now, the function may be used as:
<pre>
def my_function(1, 2, 3)
</pre>

or
<pre>
def my_function(1, 2)
</pre>

or
<pre>
def my_function(1)
</pre>

<p>
In the code the I wrote, optional parameters need strong justification. They should be optional
in the sense that correct behavior would be guaranteed even if some of the optional parameters
are missed. Perhaps they are hint for some performance optimization, or some very unusual
settings.
</p>

<p>
Once in a code that I wrote, the default values were merely some "standard" values. A non standard
value needed to be explicitly passed as argument, in absence of which standard value was used.
As time passed and the product evolved, the use case for one of the non standard value surpassed
the use case of the standard value. A new developer reasonable assumed that since the parameter
is optional, the usual (early non-standard) value would be used as default. This led to a nasty
bug, and a learning from the mistake, we abolished developers using default values unless there
was a strong justification.
</p>

<p>
The situation is a little different when writing components which face external world. Even if some external
client is not honoring the interface, not performing the functionality may be detrimental to your
business. In such cases, you may wish to log the error, and let the client know, but continue
the functionality for the time being. A typical example is Google search engines. If you type
in something to search and Google believes you probably mean something else (may be because of a typo), 
it gives you search results according to what it thought you mean, and also lets you know that.
So, Google is forgiving about the errors that external agent makes.
</p>

<p>
What is internal and what is external may depend on your company size and team dynamics. For a small
team of 4-6 people, all the company's code may be internal and outside code (libraries, API users)
would be external. For a company with many large teams, each working on a separate component, code
within each team could be internal, while code developed by two different teams could be regarded
as external.
</p>

<p>
Like all other ideas, they are general guideliness and not hard rules.
</p>
</body>
</html>
