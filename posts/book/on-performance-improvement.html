<html>
<head>
  <title>On performance improvement</title>
  <link rel="stylesheet" type="text/css" href="../style.css"/>
</head>
<body>
<h2>On performance improvement</h2>

<p>
Everyone wants superior performance for their product or service. However, unless you are methodical
about it, it is easy to spend a lot of time optimizing your code and systems, with little actual
improvement to show for the efforts. This chapter discuss some of aspects to consider when trying
to improve the performance of the systems.
</p>
<h3>Do you need superior performance?</h3>
<p>
While having a good performance is, well, good, before you take up an optimization attempt, you should
take a step back and think if you need better performance. Any time put in improving performance is
time put in doing some other activity, and thus performnace improvement efforts need to justify
themsleves relative to other activities that could be done.
</p>

<p>
This sounds obvious, but many times, engineers are driven by their insticts of aesthetics and
problem solving, rather than pragmatism. The thrill of improving software may outweight the 
pragmatic consideration whether their improvemnt is required at all.
</p>

<p>
I was once walking and on the sidewalk and saw the pedestrian signal turn green at a distance.
I committed to myself that I am going to cross the road before the signal turned red. I 
started running, and just managed to cross the road before the signal turned red. Only after
this I realized that my destination was on the other side of the signal and I never needed
to cross the road. I waited for another green signal to cross the road back. Indeed, sometimes,
overcoming the challenges becomes more important than practical considerations.
</p>

<p>
Does it really matter if your nightly job takes 30 min instead of 1 hour? Does it matter
if you reduce the memory consumption of program from 100 GB to 90GB? Would your users
appreciate if the latency of requests goes down from 20ms to 19.8ms? In many situations,
the answer is no, and you should move on to the next problem, which hopefully, matters.
</p>

<h3>Amadahl's / Gustafson's laws</h3>
<p>
Another factor to keep in mind while working on performance improvement is Amadahl's law.
Amadahl's law, reworded as Gustafson's law
--footnote--
Amadah's law and Gustafson's law are very simple facts. These guys were lucky that they
coined their laws which became so famous.  Gene Amadahl was an exciting fellow, who kept 
on founding companies till his sixties, and led a remarkable life.

states that overall speedup of a program is dependent not only on the speed up by which
you increase the latency of some portion of the program, but also on the portion of the
program which you do not improve.
</p>

<p>
Consider an example. Suppose a program's execution time is 100s. Suppose its execution can be 
divided into two parts: A and B. Part A takes 20s and Part B takes 80s. Now, if you speed up
part A by 90%, such that Part A takes only 2s, overall the program will take 82s, and thus
there is a speed up of 12% in overall program execution time. However, if you speed up 
Part B by a more moderate 50%, Part B takes 40s, leading to an overall execution time of 60s,
implying a speedup of 40%.
</p>

<p>
While you may see green pastures of optimization opportunities in parts of your system, do not
jump to work on them unless the overall impact of those improvements justifies the efforts.
</p>

<p>
Say you are writing an HTTP server. Say it takes an average of 10ms for your server to respond to
the request. You have figure out a way to reduce it by 20%, so that it will take only 8ms. 
10ms vs 8ms sounds like a handsome improvement, and your end users will appreciate a faster
website, won't they?
</p>

<p>
Well, not so fast. You need to consider that from end user's persepective, there is network
latency too: San Francisco to New York network latency is like is ~50ms, cross continent
network latency is even higher. So, you are reducing user preceived latency from 60ms to 58ms,
which is not that impressive.
</p>

<p>
Note that if you are doing 10ms to 8ms change with code optimization, without introducing
extra hardware (e.g., for caching), you may still justify your attempts if there is significant
infrastructure cost reduction. You then need to check how much money you end up saving.
</p>

<p>
Perhaps, while reviewing the code you found that there is a linear search happening on an array,
and using hash map can make search O(1). But before you implement improvement, consider the
size of the array, and how many times that search is done. Will you be improving the 
performance sizeably?
</p>

<p>
Perhaps you found that a datbase table is being queried on a certain column and there is
no index on that column. Before you create the index, consider how much does that buy you.
Is the table size large enough for the index to matter at all?
</p>

<h3>Measurement driven optimization</h3>
<p>
"Premature optimization is the root of all evil" said Edgar Dijkstra. For him to come up with
the quote, he himself would have wasted some time in premature optimization, or else seen
others wasting their time. Indeed, premature optimization is an urge that is to be resisted
continually.
</p>

<p>
So, what is full term (as opposed to premature) optimization? It is an optimization driven by
measurement. You measure, make an educated guess about the efficacy of the improvement
that you are undertaking, do the improvement, and measure again. Then you repeat the process.
</p>

<p>
The measurement consists of two parts: the metric and the profile.
</p>

The metric is usually a single
number representing the performance of an entire sub system. It could be running time of a program. 
For an HTTP server, this could
be response time. For a build system, this would be time to build a fresh build. In a throughput
oriented system, it could be how many requests per second a server can support before
CPU utilization of the server exceeds 70%. For a chat server, it could be the number of
concurrent connections supported before the response times exceed, say 10ms. For a frontend app
it could be page load time. For a caching server, it could be the hit rate.
</p>

<p>
Sometimes, the metrics could be at high level: how many batch jobs are failing per day? How many
open bug reports are there?
</p>

<p>
You often need to aggregate individual numbers to come up with a single metrics. 
In statistics, they are called "measures of central tendency". Typical mesures
of central tendecy are mean, median and mode.
For example,
while calculating the overall latency of all the requests sent to an HTTP server over a day,
you may take the average of individual latencies. While calculating aggregate metrics, while
average sounds most natural, this may not always be the case: there are instances when
90%ile value may be better, or maximum may be better.
</p>

<p>
For metrics which do not affect the end user directly, arithmetic mean is usually a good idea.
Let's say a program runs once every hour, and you want to have a metric for its performance.
An arithmetic mean of the 24 run times over the day would be a good choice. Similarly, if you
are tracking CPU utilization of a machine and taking a measurement every 5 minutes (like
you see in AWS CPU utilization dashboard), taking an average over a desired period is a good
idea. For example, if a job takes
t1, t2, ..., t24 units of time, you can take (t1 + t2 + ... + t24) / 24 as a respresentative 
running time of the program.
</p>

<p>
For some metrics, an average is an unsatisfactory measure. Let's say that you have a machine
with 4GB memory, and you run a memory heavy program on it. You measure the memory utilization
of the program at important points in your program (when you expect the program to take locally
maximum amount of memory). Say you measure the memory at 4 points in your program and find it
to be 200MB, 600MB, 800MB and 3.2GB. Then the average (200 + 600 + 800 + 3200) / 4 = 1200MB 
= 1.2GB is not a good measure of aggregate memory utilization. You should watch for max value,
since if this max gets close to 4GB, you have a problem in your hand: you either need to
move to a bigger memory machine, or optimize your program to make it use a lesser amount of
memory.
</p>

<p>
Similarly, some times, minimum rather than maximum can be appropriate value to use. For
instance, if you are tracking free memory in the system, you need to know how close to 0
do you reach, and hence tracking the min free memory of a time period is appropriate
</p>

<p>
Suppose you have written a backend to generate some report based on
user input. Depending on the user input, report generation may range from being extremely
fast to extremely slow. You want a single number to denote the aggregate performance. 
In such a situaation, neither arithemetic mean nor maximum is a suitable measure
of central tendency. Say, there are 100 requests, 99 of which took 1s while 1 request
took 1000s. Now the average response time is (99 * 1 + 1000) / 100 = 10.99s, which is
not representative since most requests are taking just 1s. Maximum is even worse.
A good measure in this case is to take some percentile value, like 95 percentile, 
or 99 percentile. If 95 percentile response time is t, it means that only 5 perecent
of the requests takes more than time t. So, you have an idea how many users are being
pained by poor performance. In the above case, 95 perecentile response time would be 1s,
which is a good measure of central tendency.
</p>

<p>
Percentile values are good to use in cases where a few abnormally large values may
skew the aggregates like mean and cause it to become unrealistic. Remember that
when Bill Gates walks in a room, the average net worth of a person in that room
is north of a billion dollars.
</p>

<h3>Some ideas to improve performance</h3>
<p>
While performnace improvements are quite application dependent, there are some general
patterns which are useful to know.
</p>

<p>
<b>Caching</b>
Caching is perhaps the most important idea for performance improvements. Caching
refers to the practice of storing the result of some computation or operation in a
quickly accessible location so that next access is faster.
</p>

<p>
Idea of caching is used pervasively in computer systems:
<ul>
<li>
Processors use memory hierarchy. L1, L2, L3 caches are on chip (or slightly off chip)
caches with store data from memory. They are fast compared to memory. Processor
utilizes the fact that most of the time you work on a small subset of data stored in
the memory, and by storing this subset in the cache, program can be run much faster.
</li>

<li>
All databases use in memory caches, in which they store data fetched from the hard disk.
</li>

<li>
File system too implements a file sytem cache. The idea is that if a part of the file is
accessed now, it is also likely to be accessed again, may be by a different program.
</li>

<li>
Virtual Machines, which translate code on run time, cache the translation to be used
at a later point of time. An example would be Java runtime environment.
</li>
<li>
Content Delivery Networks (CDN) cache static web pages across geographical nodes so as to
server static content to respective users from a location close to the end user. You can also
use a CDN to speed up the delivery of static content. Amazon's CDN, called Amazon CloudFront,
is easy to use.
</li>

<li>
The popular memoisation technique in dynamic programming algorithms is a manifestation
of caching.
</li>

<li>
Many websites cache user data so that user login is fast. In memory data stores like Redis
and Aerospike are good caching servers for such purposes.
</li>

<li>
Web browsers cache the pages that you view, so that the pages load faster next time. A web
server can set directives about how long a page can be cached.
</li>

<li>
As seen in chapter x.y.z, you can put a caching server in front of database, so that idempotent
write queries to databases are reduced, thus reduce the load on the database servers.
</li>

</ul>
</p>

<p>
<b>Precomputation</b>
Precomputation is another big idea for optimization. Instead of computing when the
need arises, we perform do the computation speculativel, and return the result immediately
when requested. This too is used across computer science.
</p>

<p>
<ul>
<li>
<p>
In processors, there is a "pipeline" which executes instructions one by one. When it encounters
an "if" instruction, it needs to take one of the two paths, depending on which way the "if"
condition turns out.
</p>
<p>
If for some reason, executing some instruction takes time (perhaps because they involve time
taking mulitplication or division, or a fetch from memory), processor continues to execute
next instructions, without waiting for previous instructions to finish (as long as input of
those instructions are ready). This is called out of order processing. 
If they encounter an "if" instruction while executing out of order, the processor makes
an educated guess (called branch prediction) about which way the branch will be resolved
and continues exeucting down that path. In case the guess turns out to be wrong, the processor
discards the results related to computation after the branch and restarts from that point.
Branch prediction and out of order execution are two of the chief mechanisms for processors
to get higher IPS (instructions per second) speed.
</p>
</li>
<p>
When a program requests the operating system to read a part of the file from hard disk to memory,
some file systems perform what is called "speculative prefetching". They fetch bytes of the
file from hard disk to memory in anticipation of requirements of the program. Speculative
prefetching is also performed by browsers in that they speculatively fetch the pages which
the expect the user to request.
</p>

<p>
Some databases implemente "materialized views", which is the result of a query computed periodically
and kept aside for quick query response time.
</p>
</ul>

<p>
Idea of precomputation may be applied in a myriad number of ways. At QGraph, we had a popular
use case where our customers segmented their user on the basis of city: for example they found
out all user the users whose city was, say, New Delhi or Bangalore. For each segmentation, querying
the database was time consuming, so we ran a nightly job which performed this segmentation.  When
a user requested the segmentation, we could fetch the precomputed results, and add to it data which
had since last night to output the final set of users.
</p>

<b>Batching</b>
<p>
Another idea to improve the performance of computer systems is batching, where 
</p>
Batching
Compressing
Indexing
Tradeoff accuracy for performance
Incremental computation

<h3>Performance can always be improved</h3>
<h3>What was luxury yesterday becomes a necessity today</h3>
</body>
</html>
