<html>
<head>
  <title>On performance improvement</title>
  <link rel="stylesheet" type="text/css" href="../style.css"/>
</head>
<body>
<h2>On performance improvement</h2>

<p>
Everyone wants superior performance for their product or service. However, unless you are methodical
about it, it is easy to spend a lot of time optimizing your code and systems, with little actual
improvement to show for the efforts. This chapter discuss some of aspects to consider when trying
to improve the performance of the systems.
</p>
<h3>Do you need superior performance?</h3>
<p>
While having a good performance is, well, good, before you take up an optimization attempt, you should
take a step back and think if you need better performance. Any time put in improving performance is
time put in doing some other activity, and thus performnace improvement efforts need to justify
themsleves relative to other activities that could be done.
</p>

<p>
This sounds obvious, but many times, engineers are driven by their insticts of aesthetics and
problem solving, rather than pragmatism. The thrill of improving software may outweight the 
pragmatic consideration whether their improvemnt is required at all.
</p>

<p>
I was once walking and on the sidewalk and saw the pedestrian signal turn green at a distance.
I committed to myself that I am going to cross the road before the signal turned red. I 
started running, and just managed to cross the road before the signal turned red. Only after
this I realized that my destination was on the other side of the signal and I never needed
to cross the road. I waited for another green signal to cross the road back. Indeed, sometimes,
overcoming the challenges becomes more important than practical considerations.
</p>

<p>
Does it really matter if your nightly job takes 30 min instead of 1 hour? Does it matter
if you reduce the memory consumption of program from 100 GB to 90GB? Would your users
appreciate if the latency of requests goes down from 20ms to 19.8ms? In many situations,
the answer is no, and you should move on to the next problem, which hopefully, matters.
</p>

<h3>Amadahl's / Gustafson's laws</h3>
<p>
Another factor to keep in mind while working on performance improvement is Amadahl's law.
Amadahl's law, reworded as Gustafson's law
--footnote--
Amadah's law and Gustafson's law are very simple facts. These guys were lucky that they
coined their laws which became so famous.  Gene Amadahl was an exciting fellow, who kept 
on founding companies till his sixties, and led a remarkable life.

states that overall speedup of a program is dependent not only on the speed up by which
you increase the latency of some portion of the program, but also on the portion of the
program which you do not improve.
</p>

<p>
Consider an example. Suppose a program's execution time is 100s. Suppose its execution can be 
divided into two parts: A and B. Part A takes 20s and Part B takes 80s. Now, if you speed up
part A by 90%, such that Part A takes only 2s, overall the program will take 82s, and thus
there is a speed up of 12% in overall program execution time. However, if you speed up 
Part B by a more moderate 50%, Part B takes 40s, leading to an overall execution time of 60s,
implying a speedup of 40%.
</p>

<p>
While you may see green pastures of optimization opportunities in parts of your system, do not
jump to work on them unless the overall impact of those improvements justifies the efforts.
</p>

<p>
Say you are writing an HTTP server. Say it takes an average of 10ms for your server to respond to
the request. You have figure out a way to reduce it by 20%, so that it will take only 8ms. 
10ms vs 8ms sounds like a handsome improvement, and your end users will appreciate a faster
website, won't they?
</p>

<p>
Well, not so fast. You need to consider that from end user's persepective, there is network
latency too: San Francisco to New York network latency is like is ~50ms, cross continent
network latency is even higher. So, you are reducing user preceived latency from 60ms to 58ms,
which is not that impressive.
</p>

<p>
Note that if you are doing 10ms to 8ms change with code optimization, without introducing
extra hardware (e.g., for caching), you may still justify your attempts if there is significant
infrastructure cost reduction. You then need to check how much money you end up saving.
</p>

<p>
Perhaps, while reviewing the code you found that there is a linear search happening on an array,
and using hash map can make search O(1). But before you implement improvement, consider the
size of the array, and how many times that search is done. Will you be improving the 
performance sizeably?
</p>

<p>
Perhaps you found that a datbase table is being queried on a certain column and there is
no index on that column. Before you create the index, consider how much does that buy you.
Is the table size large enough for the index to matter at all?
</p>

<h3>Measurement driven optimization</h3>
<p>
"Premature optimization is the root of all evil" said Edgar Dijkstra. For him to come up with
the quote, he himself would have wasted some time in premature optimization, or else seen
others wasting their time. Indeed, premature optimization is an urge that is to be resisted
continually.
</p>

<p>
So, what is full term (as opposed to premature) optimization? It is an optimization driven by
measurement. You measure, make an educated guess about the efficacy of the improvement
that you are undertaking, do the improvement, and measure again. Then you repeat the process.
</p>

<p>
The measurement consists of two parts: the metric and the profile.
</p>

The metric is usually a single
number representing the performance of an entire sub system. It could be running time of a program. 
For an HTTP server, this could
be response time. For a build system, this would be time to build a fresh build. In a throughput
oriented system, it could be how many requests per second a server can support before
CPU utilization of the server exceeds 70%. For a chat server, it could be the number of
concurrent connections supported before the response times exceed, say 10ms. For a frontend app
it could be page load time. For a caching server, it could be the hit rate.
</p>

<p>
Sometimes, the metrics could be at high level: how many batch jobs are failing per day? How many
open bug reports are there?
</p>

<p>
You often need to aggregate individual numbers to come up with a single metrics. 
In statistics, they are called "measures of central tendency". Typical mesures
of central tendecy are mean, median and mode.
For example,
while calculating the overall latency of all the requests sent to an HTTP server over a day,
you may take the average of individual latencies. While calculating aggregate metrics, while
average sounds most natural, this may not always be the case: there are instances when
90%ile value may be better, or maximum may be better.
</p>

<p>
For metrics which do not affect the end user directly, arithmetic mean is usually a good idea.
Let's say a program runs once every hour, and you want to have a metric for its performance.
An arithmetic mean of the 24 run times over the day would be a good choice. Similarly, if you
are tracking CPU utilization of a machine and taking a measurement every 5 minutes (like
you see in AWS CPU utilization dashboard), taking an average over a desired period is a good
idea. For example, if a job takes
t1, t2, ..., t24 units of time, you can take (t1 + t2 + ... + t24) / 24 as a respresentative 
running time of the program.
</p>

<p>
For some metrics, an average is an unsatisfactory measure. Let's say that you have a machine
with 4GB memory, and you run a memory heavy program on it. You measure the memory utilization
of the program at important points in your program (when you expect the program to take locally
maximum amount of memory). Say you measure the memory at 4 points in your program and find it
to be 200MB, 600MB, 800MB and 3.2GB. Then the average (200 + 600 + 800 + 3200) / 4 = 1200MB 
= 1.2GB is not a good measure of aggregate memory utilization. You should watch for max value,
since if this max gets close to 4GB, you have a problem in your hand: you either need to
move to a bigger memory machine, or optimize your program to make it use a lesser amount of
memory.
</p>

<p>
Suppose you have written a backend to generate some report based on
user input. Depending on the user input, report generation may range from being extremely
fast to extremely slow. You want a single number to denote the aggregate performance. 
In such a situaation, neither arithemetic mean nor maximum is a suitable measure
of central tendency. Say, there are 100 requests, 99 of which took 1s while 1 request
took 1000s. Now the average response time is (99 * 1 + 1000) / 100 = 10.99s, which is
not representative since most requests are taking just 1s. Maximum is even worse.
A good measure in this case is to take some percentile value, like 95 percentile, 
or 99 percentile. If 95 percentile response time is t, it means that only 5 perecent
of the requests takes more than time t. So, you have an idea how many users are being
pained by poor performance. In the above case, 95 perecentile response time would be 1s,
which is a good measure of central tendency.
</p>

<p>
Percentile values are good to use in cases where a few abnormally large values may
skew the aggregates like mean and cause it to become unrealistic. Remember that
when Bill Gates walks in a room, the average net worth of a person in that room
is north of a billion dollars.
</p>
<p>
Note that they are metrics in our control, not like active users per day.
</p>
<h3>Measuring some important metrics</h3>
<p>

</p>
<h3>Some ideas to improve performance</h3>
<h3>Performance can always be improved</h3>
<h3>What was luxury yesterday becomes a necessity today</h3>
</body>
</html>
