<html>
<head>
  <title> Notes on Logistic Regression</title>
  <link rel="stylesheet" type="text/css" href="style.css"/>
  <style>
     li {
        margin-bottom: 10px;
     }
  </style>

  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
</head>
<body>
<a href="http://thevivekpandey.github.io">Home</a>&nbsp;&nbsp; <a href="https://qgraph.io">QGraph</a>
<h1>Notes on Logistic Regression</h1>
<p>
I am doing Andrew Ng's course on deep learning, and wanted to note down the main formulae here.
</p>

<h3>Problem Formulation</h3>
We are given $(x, y)$ pairs where $x \in R^{n_x}$ and $y \in \{0, 1\}$.
<br/> <br/>
$x$ is written as a column vector.
<br/> <br/>
We want $\hat{y} = P(y = 1 \mid x)$.
<br/> <br/>
We denote those pairs as $(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}) \dots (x^{(m)}, y^{(m)}) $
<br/> <br/>
We say $\hat{y} = \sigma(w^{T}x + b)$,
<br/> <br/>
by which we mean
<br/> <br/>
$\hat{y^{(i)}} = \sigma(w^{T}x^{(i)} + b)$ for $i = 1\dots m$
<br/> <br/>
where $w \in R^{n_x}$ is again a column vector, and $b \in R$ and
$\sigma(z) = \frac{1}{1 + e^{-z}}$
<br/> <br/>

Now we need to find $w$ and $b$ such that cost function 
<br/> <br/>
$J(w,b) = \frac{1}{m} \sum_{i=1}^{m}L(\hat{y^{(i)}}, y^{i})$
<br/> <br/>
is minimized, where
<br/> <br/>
$L(\hat{y}, y) = -(ylog\hat{y} + (1 - y)log(1 - \hat{y}))$
<h3>Gradient Descent to Solve the Problem</h3>
Main idea is, start with some value of $w$ and $b$, and then repeatedly:
<br/> <br/>
$w = w - \alpha\frac{dJ(w,b)}{dw}$
<br/> <br/>
$b = b - \alpha\frac{dJ(w,b)}{db}$
<br/> <br/>
till $w$ and $b$ converge.
<br/> <br/>
Now, for one training example, let us use $\hat{y} = a$.
<br/> <br/>
$\frac{dL(a,y)}{dw} = \frac{dL(a,y)}{dz}\frac{dz}{dw}$
<br/> <br/>
where $z = \sigma({w^{T}x + b})$
<br/> <br/>
It simplifies to
<br/> <br/>
$\frac{dL(a,y)}{dw} = (-\frac{y}{a} + \frac{1-y}{1-a}) * a(1-a)$
<br/> <br/>
$= a - y$
<br/> <br/>
Thus, summing over $m$ training examples,
<br/> <br/>
\frac{dJ}{dw} = \frac{1}{m}


==================================
<br/> <br/>
Since $x$'s are written as column vector, we can introduce
<br/> <br/>
$X = 
\begin{bmatrix} 
\mid & \mid & & \mid\\
x^{(1)}& x^{(2)} & \dots & x^{(m)}\\
\mid & \mid & & \mid
\end{bmatrix}$
</body>
</html>
